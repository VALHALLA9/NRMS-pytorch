{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\anaconda3\\envs\\nrms\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import time\n",
    "from utils._constants import *\n",
    "from utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    "    ebnerd_from_path,\n",
    ")\n",
    "from evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from utils._python import (\n",
    "    write_submission_file,\n",
    "    rank_predictions_by_score,\n",
    "    write_json_file,\n",
    ")\n",
    "from utils._articles import create_article_id_to_value_mapping\n",
    "from utils._polars import split_df_chunks\n",
    "\n",
    "from models.model_config import (\n",
    "    hparams_nrms_docvec,\n",
    "    hparams_to_dict,\n",
    "    print_hparams,\n",
    ")\n",
    "from models.nrms_docvec import NRMSDocVec  \n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "from models.nrms_docvec import NRMSDocVec\n",
    "from args_nrms_docvec import get_args\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "cuDNN version: 90100\n",
      "GPU Name: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.dataloader import NRMSDataLoader, NRMSDataLoaderPretransform\n",
    "from pathlib import Path\n",
    "\n",
    "CURRENT_DIR = Path(os.getcwd())  # Get the current working directory\n",
    "\n",
    "\n",
    "PATH = Path(\".../Dataset\").resolve() \n",
    "SEED = None  \n",
    "DATASPLIT = \"ebnerd_demo\"\n",
    "DEBUG = False\n",
    "BS_TRAIN = 32\n",
    "BS_TEST = 32\n",
    "BATCH_SIZE_TEST_WO_B = 32\n",
    "BATCH_SIZE_TEST_W_B = 4\n",
    "HISTORY_SIZE = 20\n",
    "NPRATIO = 4\n",
    "EPOCHS = 5\n",
    "TRAIN_FRACTION = 1.0 if not DEBUG else 0.0001\n",
    "FRACTION_TEST = 0.001 if not DEBUG else 0.0001\n",
    "DOC_VEC_PATH = \"Dataset\\\\contrastive_vector.parquet\"\n",
    "\n",
    "\n",
    "NRMSLoader_training = NRMSDataLoaderPretransform\n",
    "\n",
    "\n",
    "\n",
    "model_func = \"NRMSDocVec\"  \n",
    "hparams = {\n",
    "    \"title_size\": 768,\n",
    "    \"history_size\": 20,\n",
    "    \"head_num\": 16,\n",
    "    \"head_dim\": 16,\n",
    "    \"attention_hidden_dim\": 200,\n",
    "    \"newsencoder_units_per_layer\": [512, 512, 512],\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"cross_entropy_loss\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"newsencoder_l2_regularization\": 1e-4,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "df_articles = pl.read_parquet(DOC_VEC_PATH)\n",
    "\n",
    "\n",
    "def create_article_id_to_value_mapping(df, value_col):\n",
    "    return {row[0]: row[1] for row in df.select([df.columns[0], value_col]).iter_rows()}\n",
    "\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=df_articles.columns[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMP_DIR = PATH.joinpath(PATH, \"DUMP\")\n",
    "DUMP_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "DT_NOW = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "MODEL_NAME = model_func\n",
    "MODEL_OUTPUT_NAME = f\"{MODEL_NAME}-{DT_NOW}\"\n",
    "\n",
    "ARTIFACT_DIR = DUMP_DIR.joinpath(\"test_predictions\", MODEL_OUTPUT_NAME)\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_WEIGHTS = DUMP_DIR.joinpath(f\"state_dict/{MODEL_OUTPUT_NAME}/weights.pt\")\n",
    "MODEL_WEIGHTS.parent.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR = DUMP_DIR.joinpath(f\"runs/{MODEL_OUTPUT_NAME}\")\n",
    "\n",
    "TEST_CHUNKS_DIR = ARTIFACT_DIR.joinpath(\"test_chunks\")\n",
    "TEST_CHUNKS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "N_CHUNKS_TEST = 1\n",
    "CHUNKS_DONE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We just want to load the necessary columns\n",
    "COLUMNS = [\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_USER_COL,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {\n",
    "    \"data_path\": str(PATH), \n",
    "    \"seed\": SEED,\n",
    "    \"datasplit\": DATASPLIT,\n",
    "    \"debug\": DEBUG,\n",
    "    \"bs_train\": BS_TRAIN,\n",
    "    \"bs_test\": BS_TEST,\n",
    "    \"batch_size_test_wo_b\": BATCH_SIZE_TEST_WO_B,\n",
    "    \"batch_size_test_w_b\": BATCH_SIZE_TEST_W_B,\n",
    "    \"history_size\": HISTORY_SIZE,\n",
    "    \"npratio\": NPRATIO,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"train_fraction\": TRAIN_FRACTION,\n",
    "    \"fraction_test\": FRACTION_TEST,\n",
    "    \"nrms_loader\": str(NRMSLoader_training), \n",
    "    \"document_embeddings\": str(DOC_VEC_PATH),  \n",
    "    \"title_size\": hparams[\"title_size\"],\n",
    "    \"head_num\": hparams[\"head_num\"],\n",
    "    \"head_dim\": hparams[\"head_dim\"],\n",
    "    \"attention_hidden_dim\": hparams[\"attention_hidden_dim\"],\n",
    "    \"newsencoder_units_per_layer\": hparams[\"newsencoder_units_per_layer\"],\n",
    "    \"optimizer\": hparams[\"optimizer\"],\n",
    "    \"loss\": hparams[\"loss\"],\n",
    "    \"dropout\": hparams[\"dropout\"],\n",
    "    \"learning_rate\": hparams[\"learning_rate\"],\n",
    "    \"newsencoder_l2_regularization\": hparams[\"newsencoder_l2_regularization\"],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "write_json_file(params_dict, ARTIFACT_DIR.joinpath(f\"{MODEL_NAME}_argparser.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = (\n",
    "    pl.concat(\n",
    "        [\n",
    "            ebnerd_from_path(\n",
    "                PATH.joinpath(DATASPLIT, \"train\"),\n",
    "                history_size=HISTORY_SIZE,\n",
    "                padding=0,\n",
    "            ),\n",
    "            ebnerd_from_path(\n",
    "                PATH.joinpath(DATASPLIT, \"validation\"),\n",
    "                history_size=HISTORY_SIZE,\n",
    "                padding=0,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    .sample(fraction=TRAIN_FRACTION, shuffle=True, seed=SEED)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=NPRATIO,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['impression_id', 'article_id', 'impression_time', 'read_time', 'scroll_percentage', 'device_type', 'article_ids_inview', 'article_ids_clicked', 'user_id', 'is_sso_user', 'gender', 'postcode', 'age', 'is_subscriber', 'session_id', 'next_read_time', 'next_scroll_percentage', 'article_id_fixed']\n",
      "['impression_id', 'article_id', 'impression_time', 'read_time', 'scroll_percentage', 'device_type', 'article_ids_inview', 'article_ids_clicked', 'user_id', 'is_sso_user', 'gender', 'postcode', 'age', 'is_subscriber', 'session_id', 'next_read_time', 'next_scroll_percentage', 'article_id_fixed']\n"
     ]
    }
   ],
   "source": [
    "train_df = ebnerd_from_path(\n",
    "    PATH.joinpath(DATASPLIT, \"train\"),\n",
    "    history_size=HISTORY_SIZE,\n",
    "    padding=0,\n",
    ")\n",
    "\n",
    "validation_df = ebnerd_from_path(\n",
    "    PATH.joinpath(DATASPLIT, \"validation\"),\n",
    "    history_size=HISTORY_SIZE,\n",
    "    padding=0,\n",
    ")\n",
    "\n",
    "print(train_df.columns)\n",
    "print(validation_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COLUMNS: ['impression_time', 'article_id_fixed', 'article_ids_inview', 'article_ids_clicked', 'impression_id', 'user_id']\n",
      "Duplicate columns in COLUMNS: []\n"
     ]
    }
   ],
   "source": [
    "print(\"COLUMNS:\", COLUMNS)\n",
    "print(\"Duplicate columns in COLUMNS:\", [col for col in COLUMNS if COLUMNS.count(col) > 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_dt = df[DEFAULT_IMPRESSION_TIMESTAMP_COL].dt.date().max() - dt.timedelta(days=1)\n",
    "df_train = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL).dt.date() < last_dt)\n",
    "df_validation = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL).dt.date() >= last_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSLoader_training type: <class 'type'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"NRMSLoader_training type: {type(NRMSLoader_training)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NRMSLoader_training(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=BS_TRAIN,\n",
    ")\n",
    "val_dataset = NRMSLoader_training(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=BS_TRAIN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=None, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=None, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = NRMSDocVec(hparams=hparams_nrms_docvec, seed=42)  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.model.to(device)\n",
    "model.scorer.to(device)\n",
    "optimizer = model.optimizer \n",
    "criterion = model.criterion  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=4, mode='max', restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.best_state_dict = None\n",
    "\n",
    "    def step(self, score, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_state_dict = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False\n",
    "        improve = (score > self.best_score) if self.mode == 'max' else (score < self.best_score)\n",
    "        if improve:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            self.best_state_dict = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                if self.restore_best_weights:\n",
    "                    model.load_state_dict(self.best_state_dict)\n",
    "                return True\n",
    "            return False\n",
    "# early_stopping = EarlyStopping(monitor=\"val_auc\", mode=\"max\", patience=4, restore_best_weights=True)\n",
    "early_stopping = EarlyStopping(patience=4, mode=\"max\", restore_best_weights=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=2, min_lr=1e-6)\n",
    "\n",
    "def analyze_time_impact(model, test_loader, device):\n",
    "    \"\"\"time\"\"\"\n",
    "    model.eval()\n",
    "    time_effects = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            if len(batch[0]) == 4:\n",
    "                (his_input, pred_input, his_time, pred_time), _ = batch\n",
    "\n",
    "                pred_with_time = model(his_input, pred_input, his_time, pred_time)\n",
    "                pred_without_time = model(his_input, pred_input)\n",
    "                \n",
    "                time_effect = torch.abs(pred_with_time - pred_without_time).mean()\n",
    "                time_effects.append(time_effect.item())\n",
    "    \n",
    "    return np.mean(time_effects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=4, mode=\"max\", restore_best_weights=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=2, min_lr=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc(model, dataloader, device):\n",
    "\n",
    "    model.scorer.eval()\n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(dataloader, desc=\"Computing validation metrics\", \n",
    "                          dynamic_ncols=True, leave=True)\n",
    "        \n",
    "        for (his_input_title, pred_input_title), batch_y in progress_bar:\n",
    "            his_input_title = his_input_title.to(dtype=torch.float32, device=device)\n",
    "            pred_input_title = pred_input_title.to(dtype=torch.float32, device=device)\n",
    "            batch_y = batch_y.to(dtype=torch.float32, device=device)\n",
    "            \n",
    "\n",
    "            pred_input_title_one = pred_input_title[:, 0:1, :]\n",
    "            scores = model.scorer(his_input_title, pred_input_title_one)\n",
    "            \n",
    "\n",
    "            scores = scores.cpu().numpy()\n",
    "            labels = batch_y[:, 0].cpu().numpy()\n",
    "            \n",
    "            all_scores.extend(scores)\n",
    "            all_labels.extend(labels)\n",
    "    \n",
    "\n",
    "    metrics_dict = {}\n",
    "    auc_metric = AucScore()\n",
    "    mrr_metric = MrrScore()\n",
    "    ndcg_5_metric = NdcgScore(k=5)\n",
    "    ndcg_10_metric = NdcgScore(k=10)\n",
    "    \n",
    "    metrics_dict['auc'] = auc_metric.calculate([all_labels], [all_scores])\n",
    "    metrics_dict['mrr'] = mrr_metric.calculate([all_labels], [all_scores])\n",
    "    metrics_dict['ndcg@5'] = ndcg_5_metric.calculate([all_labels], [all_scores])\n",
    "    metrics_dict['ndcg@10'] = ndcg_10_metric.calculate([all_labels], [all_scores])\n",
    "    \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.model.train()\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    progress_bar = tqdm(total=len(dataloader), desc=\"Training\", dynamic_ncols=True)\n",
    "\n",
    "    for batch_idx, ((his_input_title, pred_input_title), batch_y) in enumerate(dataloader):\n",
    "\n",
    "        # if not isinstance(his_input_title, torch.Tensor):\n",
    "        #     his_input_title = torch.from_numpy(his_input_title).float()\n",
    "        # his_input_title = his_input_title.to(device)\n",
    "\n",
    "        # if not isinstance(pred_input_title, torch.Tensor):\n",
    "        #     pred_input_title = torch.from_numpy(pred_input_title).float()\n",
    "        # pred_input_title = pred_input_title.to(device)\n",
    "\n",
    "        # if not isinstance(batch_y, torch.Tensor):\n",
    "        #     batch_y = torch.from_numpy(batch_y).float()\n",
    "        # batch_y = batch_y.to(device)\n",
    "\n",
    "\n",
    "        his_input_title = his_input_title.to(dtype=torch.float32, device=device)\n",
    "        pred_input_title = pred_input_title.to(dtype=torch.float32, device=device)\n",
    "        batch_y = batch_y.to(dtype=torch.float32, device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model.model(his_input_title, pred_input_title) \n",
    "        \n",
    "        # categorical_crossentropy\n",
    "        loss = -torch.sum(batch_y * torch.log(preds + 1e-10)) / batch_y.size(0)\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        grad_stats = []\n",
    "        for name, param in model.model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_stats.append(f\"{name}: grad_mean={param.grad.mean().item():.6f}\")\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * len(batch_y)\n",
    "        count += len(batch_y)\n",
    "\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            loss=f\"{loss.item():.6f}\",\n",
    "            grad_stats=\" | \".join(grad_stats[:2])\n",
    "        )\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    progress_bar.close()\n",
    "    return total_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating NRMSDocVec, start training...\n",
      "\n",
      "Epoch 1/5\n",
      "Current learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1426/1426 [00:19<00:00, 72.86it/s, grad_stats=userencoder.newsencoder.layers.0.0.weight: grad_mean=0.000021 | userencoder.newsencoder.layers.0.0.bias: grad_mean=0.007871, loss=1.303622]  \n",
      "Evaluating: 100%|██████████| 150/150 [00:01<00:00, 122.39it/s, loss=1.835554]\n",
      "AUC: 100%|████████████████████████████████| 4779/4779 [00:03<00:00, 1466.56it/s]\n",
      "AUC: 100%|██████████████████████████████| 4779/4779 [00:00<00:00, 101971.71it/s]\n",
      "AUC: 100%|███████████████████████████████| 4779/4779 [00:00<00:00, 50258.20it/s]\n",
      "AUC: 100%|███████████████████████████████| 4779/4779 [00:00<00:00, 51026.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "Train Loss: 1.5339\n",
      "Val Loss: 1.5150\n",
      "Val AUC: 0.6218\n",
      "Val MRR: 0.5721\n",
      "Val NDCG@5: 0.6774\n",
      "Val NDCG@10: 0.6774\n",
      "\n",
      "Best model updated\n",
      "\n",
      "Epoch 2/5\n",
      "Current learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1426/1426 [00:19<00:00, 73.35it/s, grad_stats=userencoder.newsencoder.layers.0.0.weight: grad_mean=0.000016 | userencoder.newsencoder.layers.0.0.bias: grad_mean=0.004080, loss=1.432175]  \n",
      "Evaluating: 100%|██████████| 150/150 [00:01<00:00, 124.83it/s, loss=1.770083]\n",
      "AUC: 100%|████████████████████████████████| 4779/4779 [00:03<00:00, 1483.51it/s]\n",
      "AUC: 100%|██████████████████████████████| 4779/4779 [00:00<00:00, 102875.54it/s]\n",
      "AUC: 100%|███████████████████████████████| 4779/4779 [00:00<00:00, 51178.91it/s]\n",
      "AUC: 100%|███████████████████████████████| 4779/4779 [00:00<00:00, 48395.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/5\n",
      "Train Loss: 1.4609\n",
      "Val Loss: 1.5057\n",
      "Val AUC: 0.6252\n",
      "Val MRR: 0.5776\n",
      "Val NDCG@5: 0.6815\n",
      "Val NDCG@10: 0.6815\n",
      "\n",
      "Best model updated\n",
      "\n",
      "Epoch 3/5\n",
      "Current learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1426/1426 [00:18<00:00, 75.42it/s, grad_stats=userencoder.newsencoder.layers.0.0.weight: grad_mean=-0.000026 | userencoder.newsencoder.layers.0.0.bias: grad_mean=-0.008782, loss=1.369875]\n",
      "Evaluating: 100%|██████████| 150/150 [00:01<00:00, 126.94it/s, loss=1.845830]\n",
      "AUC: 100%|████████████████████████████████| 4779/4779 [00:03<00:00, 1439.28it/s]\n",
      "AUC: 100%|██████████████████████████████| 4779/4779 [00:00<00:00, 103526.42it/s]\n",
      "AUC: 100%|███████████████████████████████| 4779/4779 [00:00<00:00, 46722.54it/s]\n",
      "AUC: 100%|███████████████████████████████| 4779/4779 [00:00<00:00, 50189.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/5\n",
      "Train Loss: 1.4240\n",
      "Val Loss: 1.5132\n",
      "Val AUC: 0.6383\n",
      "Val MRR: 0.5874\n",
      "Val NDCG@5: 0.6892\n",
      "Val NDCG@10: 0.6892\n",
      "\n",
      "Best model updated\n",
      "\n",
      "Epoch 4/5\n",
      "Current learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1426/1426 [00:19<00:00, 74.33it/s, grad_stats=userencoder.newsencoder.layers.0.0.weight: grad_mean=-0.000016 | userencoder.newsencoder.layers.0.0.bias: grad_mean=-0.006106, loss=1.296943]\n",
      "Evaluating: 100%|██████████| 150/150 [00:01<00:00, 121.37it/s, loss=1.768713]\n",
      "AUC: 100%|████████████████████████████████| 4779/4779 [00:03<00:00, 1471.84it/s]\n",
      "AUC: 100%|██████████████████████████████| 4779/4779 [00:00<00:00, 103018.82it/s]\n",
      "AUC: 100%|███████████████████████████████| 4779/4779 [00:00<00:00, 45150.13it/s]\n",
      "AUC: 100%|███████████████████████████████| 4779/4779 [00:00<00:00, 50123.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/5\n",
      "Train Loss: 1.4010\n",
      "Val Loss: 1.5273\n",
      "Val AUC: 0.6354\n",
      "Val MRR: 0.5848\n",
      "Val NDCG@5: 0.6871\n",
      "Val NDCG@10: 0.6871\n",
      "\n",
      "Epoch 5/5\n",
      "Current learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1426/1426 [00:19<00:00, 74.14it/s, grad_stats=userencoder.newsencoder.layers.0.0.weight: grad_mean=0.000002 | userencoder.newsencoder.layers.0.0.bias: grad_mean=0.000471, loss=1.451776]  \n",
      "Evaluating: 100%|██████████| 150/150 [00:02<00:00, 70.78it/s, loss=1.964568] \n",
      "AUC: 100%|████████████████████████████████| 4779/4779 [00:03<00:00, 1485.18it/s]\n",
      "AUC: 100%|██████████████████████████████| 4779/4779 [00:00<00:00, 101850.46it/s]\n",
      "AUC: 100%|███████████████████████████████| 4779/4779 [00:00<00:00, 50741.40it/s]\n",
      "AUC: 100%|███████████████████████████████| 4779/4779 [00:00<00:00, 48152.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/5\n",
      "Train Loss: 1.3775\n",
      "Val Loss: 1.5764\n",
      "Val AUC: 0.6277\n",
      "Val MRR: 0.5806\n",
      "Val NDCG@5: 0.6838\n",
      "Val NDCG@10: 0.6838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.model.eval()\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    all_labels = []\n",
    "    all_scores = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "\n",
    "        progress_bar = tqdm(total=len(dataloader), desc=\"Evaluating\", dynamic_ncols=True, leave=True)\n",
    "        for batch_idx, batch_data in enumerate(dataloader):\n",
    "            if len(batch_data[0]) == 4:  \n",
    "                (his_input_title, pred_input_title, his_time_delta, pred_time_delta), batch_y = batch_data\n",
    "            else:  \n",
    "                (his_input_title, pred_input_title), batch_y = batch_data\n",
    "                his_time_delta, pred_time_delta = None, None\n",
    "\n",
    "\n",
    "            his_input_title = his_input_title.to(dtype=torch.float32, device=device)\n",
    "            pred_input_title = pred_input_title.to(dtype=torch.float32, device=device)\n",
    "            batch_y = batch_y.to(dtype=torch.float32, device=device)\n",
    "\n",
    "            if his_time_delta is not None:\n",
    "                his_time_delta = his_time_delta.to(dtype=torch.float32, device=device)\n",
    "                pred_time_delta = pred_time_delta.to(dtype=torch.float32, device=device)\n",
    "                preds = model.model(his_input_title, pred_input_title, his_time_delta, pred_time_delta)\n",
    "            else:\n",
    "                preds = model.model(his_input_title, pred_input_title)\n",
    "\n",
    "            loss = -torch.sum(batch_y * torch.log(preds + 1e-10)) / batch_y.size(0)\n",
    "            total_loss += loss.item() * len(batch_y)\n",
    "            count += len(batch_y)\n",
    "            \n",
    "            for i in range(batch_y.size(0)):\n",
    "                all_labels.append(batch_y[i].cpu().numpy())\n",
    "                all_scores.append(preds[i].cpu().numpy())\n",
    "            \n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.6f}\")\n",
    "            progress_bar.update(1)\n",
    "        progress_bar.close()\n",
    "\n",
    "    metrics_dict = {}\n",
    "    auc_metric = AucScore()\n",
    "    mrr_metric = MrrScore()\n",
    "    ndcg_5_metric = NdcgScore(k=5)\n",
    "    ndcg_10_metric = NdcgScore(k=10)\n",
    "    \n",
    "    try:\n",
    "        metrics_dict['auc'] = auc_metric.calculate(all_labels, all_scores)\n",
    "        metrics_dict['mrr'] = mrr_metric.calculate(all_labels, all_scores)\n",
    "        metrics_dict['ndcg@5'] = ndcg_5_metric.calculate(all_labels, all_scores)\n",
    "        metrics_dict['ndcg@10'] = ndcg_10_metric.calculate(all_labels, all_scores)\n",
    "    except Exception as e:\n",
    "        print(\"\\nError in metric calculation:\", str(e))\n",
    "        print(\"Example label:\", all_labels[0])\n",
    "        print(\"Example score:\", all_scores[0])\n",
    "\n",
    "    return total_loss / count, metrics_dict\n",
    "\n",
    "\n",
    "best_auc = -1\n",
    "print(f\"Initiating {MODEL_NAME}, start training...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f\"Current learning rate: {param_group['lr']}\")\n",
    "\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss, val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"Val AUC: {val_metrics.get('auc', 0.0):.4f}\")\n",
    "    print(f\"Val MRR: {val_metrics.get('mrr', 0.0):.4f}\")\n",
    "    print(f\"Val NDCG@5: {val_metrics.get('ndcg@5', 0.0):.4f}\")\n",
    "    print(f\"Val NDCG@10: {val_metrics.get('ndcg@10', 0.0):.4f}\")\n",
    "    \n",
    "    if val_metrics.get('auc', 0.0) > best_auc:\n",
    "        best_auc = val_metrics.get('auc', 0.0)\n",
    "        torch.save(model.model.state_dict(), MODEL_WEIGHTS)\n",
    "        print(\"\\nBest model updated\")\n",
    "\n",
    "    scheduler.step(val_metrics.get('auc', 0.0))\n",
    "    stop = early_stopping.step(val_metrics.get('auc', 0.0), model.model)\n",
    "    if stop:\n",
    "        print(\"\\nEarly stopping triggered.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "cuDNN version: 90100\n",
      "GPU Name: NVIDIA GeForce RTX 3060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article mapping example:\n",
      "Key: 3000022, Value: [-0.012159083038568497, 0.05709662660956383, 0.018299145624041557, -0.03888377919793129, -0.010862666182219982]\n",
      "Key: 3000063, Value: [0.034481510519981384, 0.03353268280625343, 0.05459773540496826, -0.023162858560681343, 0.009086905978620052]\n",
      "Key: 3000613, Value: [-0.014638329856097698, 0.030934402719140053, 0.036162927746772766, 0.039488889276981354, -0.030487006530165672]\n",
      "Key: 3000700, Value: [-0.06416679173707962, 0.00485263392329216, 0.013270833529531956, -0.00036373414332047105, 0.001436168560758233]\n",
      "Key: 3000840, Value: [-0.013040119782090187, 0.0245132464915514, 0.031050924211740494, 0.012360169552266598, -0.04919935017824173]\n",
      "Key: 3001278, Value: [0.0048789093270897865, 0.01565060019493103, 0.046486884355545044, 0.05465223267674446, -0.056941013783216476]\n"
     ]
    }
   ],
   "source": [
    "print(\"Article mapping example:\")\n",
    "for i, (key, value) in enumerate(article_mapping.items()):\n",
    "    print(f\"Key: {key}, Value: {value[:5]}\")  \n",
    "    if i >= 5:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model: E:\\02456_Assignment\\pytorch\\test\\Dataset\\DUMP\\state_dict\\NRMSDocVec-2024-12-09_00-42-25\\weights.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cloud\\AppData\\Local\\Temp\\ipykernel_11756\\3854032881.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.model.load_state_dict(torch.load(MODEL_WEIGHTS, map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NRMScorer(\n",
       "  (userencoder): UserEncoder(\n",
       "    (newsencoder): NewsEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1-2): 2 x Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (output_layer): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (att_layer): AttLayer2()\n",
       "  )\n",
       "  (newsencoder): NewsEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (1-2): 2 x Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (output_layer): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"loading model: {MODEL_WEIGHTS}\")\n",
    "model.model.load_state_dict(torch.load(MODEL_WEIGHTS, map_location=device))\n",
    "model.model.eval()\n",
    "model.scorer.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set\n",
    "We'll use the validation set, as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating testset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Initiating testset...\")\n",
    "df_test = (\n",
    "    ebnerd_from_path(\n",
    "        PATH.joinpath(DATASPLIT, \"validation\"),\n",
    "        history_size=HISTORY_SIZE,\n",
    "        padding=0,\n",
    "    )\n",
    "    .sample(fraction=FRACTION_TEST)\n",
    "    .with_columns([\n",
    "        pl.col(\"article_ids_clicked\").alias(DEFAULT_CLICKED_ARTICLES_COL),\n",
    "        pl.col(\"article_ids_inview\").alias(DEFAULT_INVIEW_ARTICLES_COL),\n",
    "        pl.lit(False).alias(DEFAULT_IS_BEYOND_ACCURACY_COL)\n",
    "    ])\n",
    "    .select(COLUMNS + [DEFAULT_IS_BEYOND_ACCURACY_COL])\n",
    ")\n",
    "\n",
    "df_test = df_test.pipe(create_binary_labels_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating list lengths after padding...\n",
      "Unique list lengths: shape: (11,)\n",
      "Series: 'article_ids_inview' [u32]\n",
      "[\n",
      "\t5\n",
      "\t6\n",
      "\t7\n",
      "\t9\n",
      "\t10\n",
      "\t…\n",
      "\t12\n",
      "\t13\n",
      "\t14\n",
      "\t15\n",
      "\t27\n",
      "]\n",
      "Padding failed: Inconsistent list lengths detected.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Validating list lengths after padding...\")\n",
    "list_lengths = df_test[DEFAULT_INVIEW_ARTICLES_COL].list.len().unique()\n",
    "print(f\"Unique list lengths: {list_lengths}\")\n",
    "\n",
    "if len(list_lengths) == 1:\n",
    "    print(\"Padding successful: All lists have the same length.\")\n",
    "else:\n",
    "    print(\"Padding failed: Inconsistent list lengths detected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_scores(model_scorer, dataloader, device):\n",
    "    model_scorer.eval()\n",
    "    preds_all = []\n",
    "    \n",
    "    for (his_input_title, pred_input_title_one), _ in dataloader:\n",
    "\n",
    "        if not isinstance(his_input_title, torch.Tensor):\n",
    "            his_input_title = torch.from_numpy(his_input_title).to(dtype=torch.float32)\n",
    "        elif his_input_title.dtype != torch.float32:\n",
    "            his_input_title = his_input_title.to(dtype=torch.float32)\n",
    "        his_input_title = his_input_title.to(device)\n",
    "\n",
    "        if not isinstance(pred_input_title_one, torch.Tensor):\n",
    "            pred_input_title_one = torch.from_numpy(pred_input_title_one).to(dtype=torch.float32)\n",
    "        elif pred_input_title_one.dtype != torch.float32:\n",
    "            pred_input_title_one = pred_input_title_one.to(dtype=torch.float32)\n",
    "        pred_input_title_one = pred_input_title_one.to(device)\n",
    "\n",
    "        scores = model_scorer(his_input_title, pred_input_title_one)\n",
    "        preds_all.extend(scores.cpu().tolist())\n",
    "    \n",
    "    return np.array(preds_all, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating testset...\n",
      "Current PATH: E:\\02456_Assignment\\pytorch\\test\\Dataset\n",
      "Training data path: E:\\02456_Assignment\\pytorch\\test\\Dataset\\ebnerd_demo\\train\n",
      "Validation data path: E:\\02456_Assignment\\pytorch\\test\\Dataset\\ebnerd_demo\\validation\n",
      "Loading test data from: E:\\02456_Assignment\\pytorch\\test\\Dataset\\ebnerd_demo\\validation\n",
      "Path exists: True\n",
      "Processing test chunks...\n",
      "Processing chunk 1/1\n",
      "Scores shape: (309,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Initiating testset...\")\n",
    "df_test = (\n",
    "    ebnerd_from_path(\n",
    "        PATH.joinpath(DATASPLIT, \"validation\"),\n",
    "        history_size=HISTORY_SIZE,\n",
    "        padding=0,\n",
    "    )\n",
    "    .sample(fraction=FRACTION_TEST)\n",
    "    .with_columns([\n",
    "        pl.col(\"article_ids_clicked\").alias(DEFAULT_CLICKED_ARTICLES_COL),\n",
    "        pl.col(\"article_ids_inview\").alias(DEFAULT_INVIEW_ARTICLES_COL),\n",
    "        pl.lit(False).alias(DEFAULT_IS_BEYOND_ACCURACY_COL)\n",
    "    ])\n",
    "    .select(COLUMNS + [DEFAULT_IS_BEYOND_ACCURACY_COL])\n",
    ")\n",
    "\n",
    "df_test = df_test.pipe(create_binary_labels_column)\n",
    "\n",
    "print(\"Current PATH:\", PATH)\n",
    "print(\"Training data path:\", PATH.joinpath(DATASPLIT, \"train\"))\n",
    "print(\"Validation data path:\", PATH.joinpath(DATASPLIT, \"validation\"))\n",
    "\n",
    "test_path = PATH.joinpath(DATASPLIT, \"validation\")\n",
    "print(f\"Loading test data from: {test_path}\")\n",
    "print(f\"Path exists: {test_path.exists()}\")\n",
    "\n",
    "\n",
    "df_test_chunks = split_df_chunks(df_test, n_chunks=N_CHUNKS_TEST)\n",
    "df_pred_chunks = []\n",
    "\n",
    "print(\"Processing test chunks...\")\n",
    "for i, df_test_chunk in enumerate(df_test_chunks[CHUNKS_DONE:], start=1 + CHUNKS_DONE):\n",
    "    print(f\"Processing chunk {i}/{len(df_test_chunks)}\")\n",
    "    test_dataloader = NRMSDataLoader(\n",
    "        behaviors=df_test_chunk,\n",
    "        article_dict=article_mapping,\n",
    "        unknown_representation=\"zeros\",\n",
    "        history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "        eval_mode=True,\n",
    "        batch_size=BATCH_SIZE_TEST_WO_B,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataloader, batch_size=None, shuffle=False)\n",
    "    \n",
    "\n",
    "    try:\n",
    "        scores = predict_scores(model.scorer, test_loader, device)\n",
    "        print(f\"Scores shape: {scores.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in predict_scores: {str(e)}\")\n",
    "        first_batch = next(iter(test_loader))\n",
    "        print(f\"First batch types: {[type(x) for x in first_batch[0]]}\")\n",
    "        print(f\"First batch dtypes: {[x.dtype if isinstance(x, torch.Tensor) else None for x in first_batch[0]]}\")\n",
    "        raise e\n",
    "    \n",
    "    df_test_chunk = add_prediction_scores(df_test_chunk, scores.tolist())\n",
    "    df_test_chunk = df_test_chunk.with_columns([\n",
    "        pl.col(\"scores\")\n",
    "        .map_elements(lambda x: list(rank_predictions_by_score(x)), return_dtype=pl.List(pl.Float32))\n",
    "        .alias(\"ranked_scores\")\n",
    "    ])\n",
    "    \n",
    "    df_test_chunk.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "        TEST_CHUNKS_DIR.joinpath(f\"pred_{i}.parquet\")\n",
    "    )\n",
    "    df_pred_chunks.append(df_test_chunk)\n",
    "    del df_test_chunk, test_dataloader, scores\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (15, 2)\n",
      "┌────────────────────┬───────┐\n",
      "│ article_ids_inview ┆ count │\n",
      "│ ---                ┆ ---   │\n",
      "│ u32                ┆ u32   │\n",
      "╞════════════════════╪═══════╡\n",
      "│ 33                 ┆ 1     │\n",
      "│ 14                 ┆ 1     │\n",
      "│ 15                 ┆ 2     │\n",
      "│ 11                 ┆ 3     │\n",
      "│ 16                 ┆ 1     │\n",
      "│ …                  ┆ …     │\n",
      "│ 13                 ┆ 2     │\n",
      "│ 23                 ┆ 1     │\n",
      "│ 17                 ┆ 1     │\n",
      "│ 8                  ┆ 2     │\n",
      "│ 10                 ┆ 2     │\n",
      "└────────────────────┴───────┘\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for chunk in df_test_chunks:\n",
    "    print(chunk[DEFAULT_INVIEW_ARTICLES_COL].list.len().value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examining test chunk data structure...\n",
      "Sample test chunk head:\n",
      "shape: (5, 8)\n",
      "┌────────────┬────────────┬────────────┬────────────┬────────────┬─────────┬───────────┬───────────┐\n",
      "│ impression ┆ article_id ┆ article_id ┆ article_id ┆ impression ┆ user_id ┆ is_beyond ┆ labels    │\n",
      "│ _time      ┆ _fixed     ┆ s_inview   ┆ s_clicked  ┆ _id        ┆ ---     ┆ _accuracy ┆ ---       │\n",
      "│ ---        ┆ ---        ┆ ---        ┆ ---        ┆ ---        ┆ u32     ┆ ---       ┆ list[i8]  │\n",
      "│ datetime[μ ┆ list[i32]  ┆ list[i32]  ┆ list[i32]  ┆ u32        ┆         ┆ bool      ┆           │\n",
      "│ s]         ┆            ┆            ┆            ┆            ┆         ┆           ┆           │\n",
      "╞════════════╪════════════╪════════════╪════════════╪════════════╪═════════╪═══════════╪═══════════╡\n",
      "│ 2023-05-25 ┆ [9778302,  ┆ [9780561,  ┆ [9780428]  ┆ 269627573  ┆ 626652  ┆ false     ┆ [0, 0, …  │\n",
      "│ 11:12:45   ┆ 9777582, … ┆ 8934043, … ┆            ┆            ┆         ┆           ┆ 0]        │\n",
      "│            ┆ 9780020]   ┆ 9780482]   ┆            ┆            ┆         ┆           ┆           │\n",
      "│ 2023-05-29 ┆ [9779538,  ┆ [9782993,  ┆ [9781086]  ┆ 223410310  ┆ 1697436 ┆ false     ┆ [0, 0, …  │\n",
      "│ 20:14:02   ┆ 9779511, … ┆ 9786567, … ┆            ┆            ┆         ┆           ┆ 0]        │\n",
      "│            ┆ 9766434]   ┆ 9787116]   ┆            ┆            ┆         ┆           ┆           │\n",
      "│ 2023-05-26 ┆ [9777505,  ┆ [9783004,  ┆ [9780517]  ┆ 343060555  ┆ 43932   ┆ false     ┆ [0, 1, …  │\n",
      "│ 18:40:10   ┆ 9778219, … ┆ 9780517, … ┆            ┆            ┆         ┆           ┆ 0]        │\n",
      "│            ┆ 9779417]   ┆ 9782517]   ┆            ┆            ┆         ┆           ┆           │\n",
      "│ 2023-05-28 ┆ [9774383,  ┆ [9779807,  ┆ [9784804]  ┆ 64511955   ┆ 1686565 ┆ false     ┆ [0, 0, …  │\n",
      "│ 07:21:39   ┆ 9770028, … ┆ 9783993, … ┆            ┆            ┆         ┆           ┆ 0]        │\n",
      "│            ┆ 9776332]   ┆ 9784702]   ┆            ┆            ┆         ┆           ┆           │\n",
      "│ 2023-05-26 ┆ [9775939,  ┆ [9782672,  ┆ [9782467]  ┆ 170300033  ┆ 930903  ┆ false     ┆ [0, 0, …  │\n",
      "│ 15:26:58   ┆ 9775964, … ┆ 9780718, … ┆            ┆            ┆         ┆           ┆ 0]        │\n",
      "│            ┆ 9779577]   ┆ 9778796]   ┆            ┆            ┆         ┆           ┆           │\n",
      "└────────────┴────────────┴────────────┴────────────┴────────────┴─────────┴───────────┴───────────┘\n",
      "\n",
      "Column types:\n",
      "[Datetime(time_unit='us', time_zone=None), List(Int32), List(Int32), List(Int32), UInt32, UInt32, Boolean, List(Int8)]\n",
      "\n",
      "Sample inview articles:\n",
      "shape: (10,)\n",
      "Series: 'article_ids_inview' [list[i32]]\n",
      "[\n",
      "\t[9780561, 8934043, … 9780482]\n",
      "\t[9782993, 9786567, … 9787116]\n",
      "\t[9783004, 9780517, … 9782517]\n",
      "\t[9779807, 9783993, … 9784702]\n",
      "\t[9782672, 9780718, … 9778796]\n",
      "\t[9780702, 9789418, … 9777339]\n",
      "\t[9788323, 9787022, … 9788125]\n",
      "\t[9275787, 9779777, … 8560195]\n",
      "\t[9783057, 9777983, … 9785298]\n",
      "\t[9786618, 9786222, … 8054212]\n",
      "]\n",
      "\n",
      "Processing test chunks...\n",
      "\n",
      "Processing chunk 1/1\n",
      "Chunk shape: (25, 8)\n",
      "Scores shape: (309,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Examining test chunk data structure...\")\n",
    "sample_chunk = df_test_chunks[0]  \n",
    "print(\"Sample test chunk head:\")\n",
    "print(sample_chunk.head(5))\n",
    "print(\"\\nColumn types:\")\n",
    "print(sample_chunk.dtypes)\n",
    "print(\"\\nSample inview articles:\")\n",
    "print(sample_chunk[DEFAULT_INVIEW_ARTICLES_COL].head())\n",
    "\n",
    "print(\"\\nProcessing test chunks...\")\n",
    "for i, df_test_chunk in enumerate(df_test_chunks[CHUNKS_DONE:], start=1 + CHUNKS_DONE):\n",
    "    print(f\"\\nProcessing chunk {i}/{len(df_test_chunks)}\")\n",
    "    print(f\"Chunk shape: {df_test_chunk.shape}\")\n",
    "    \n",
    "    test_dataloader = NRMSDataLoader(\n",
    "        behaviors=df_test_chunk,\n",
    "        article_dict=article_mapping,\n",
    "        unknown_representation=\"zeros\",\n",
    "        history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "        eval_mode=True,\n",
    "        batch_size=BATCH_SIZE_TEST_WO_B,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataloader, batch_size=None, shuffle=False)\n",
    "    \n",
    "    try:\n",
    "        scores = predict_scores(model.scorer, test_loader, device)\n",
    "        print(f\"Scores shape: {scores.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error in predict_scores: {str(e)}\")\n",
    "        first_batch = next(iter(test_loader))\n",
    "        print(f\"First batch types: {[type(x) for x in first_batch[0]]}\")\n",
    "        print(f\"First batch dtypes: {[x.dtype if isinstance(x, torch.Tensor) else None for x in first_batch[0]]}\")\n",
    "        raise e\n",
    "    \n",
    "    df_test_chunk = add_prediction_scores(df_test_chunk, scores.tolist())\n",
    "    df_test_chunk = df_test_chunk.with_columns([\n",
    "        pl.col(\"scores\")\n",
    "        .map_elements(lambda x: list(rank_predictions_by_score(x)), return_dtype=pl.List(pl.Float32))\n",
    "        .alias(\"ranked_scores\")\n",
    "    ])\n",
    "    \n",
    "    df_test_chunk.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "        TEST_CHUNKS_DIR.joinpath(f\"pred_{i}.parquet\")\n",
    "    )\n",
    "    df_pred_chunks.append(df_test_chunk)\n",
    "    del df_test_chunk, test_dataloader, scores\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking test data distribution...\n",
      "Unique values in DEFAULT_CLICKED_ARTICLES_COL:\n",
      "shape: (25,)\n",
      "Series: 'article_ids_clicked' [list[i32]]\n",
      "[\n",
      "\t[9783720, 9783720]\n",
      "\t[9782407]\n",
      "\t[9777822]\n",
      "\t[9788116]\n",
      "\t[9778796]\n",
      "\t…\n",
      "\t[9781257]\n",
      "\t[9780925]\n",
      "\t[9789745]\n",
      "\t[9788666]\n",
      "\t[9780702]\n",
      "]\n",
      "Unique values in DEFAULT_INVIEW_ARTICLES_COL:\n",
      "shape: (25,)\n",
      "Series: 'article_ids_inview' [list[i32]]\n",
      "[\n",
      "\t[9785107, 9784947, … 9785267]\n",
      "\t[9783137, 9783278, … 9782517]\n",
      "\t[9785339, 9782869, … 9783740]\n",
      "\t[9782879, 9782672, … 9782616]\n",
      "\t[9781932, 9786763, … 9786906]\n",
      "\t…\n",
      "\t[9788116, 9789141, … 9789433]\n",
      "\t[9776322, 9778796, … 9780476]\n",
      "\t[9784607, 9784559, … 9783852]\n",
      "\t[9779659, 9781870, … 9781878]\n",
      "\t[9789279, 9788947, … 9771224]\n",
      "]\n",
      "\n",
      "Processing test chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|██████████| 1/1 [00:16<00:00, 16.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merging prediction results...\n",
      "Saving final predictions...\n",
      "\n",
      "Evaluating test set predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AUC: 100%|█████████████████████████████████████| 25/25 [00:00<00:00, 793.82it/s]\n",
      "AUC: 100%|███████████████████████████████████| 25/25 [00:00<00:00, 25007.77it/s]\n",
      "AUC: 100%|███████████████████████████████████| 25/25 [00:00<00:00, 24989.90it/s]\n",
      "AUC: 100%|███████████████████████████████████| 25/25 [00:00<00:00, 25055.58it/s]\n",
      "Computing metrics: 100%|██████████| 3/3 [00:00<00:00, 54.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Results:\n",
      "AUC: 0.6618\n",
      "MRR: 0.3706\n",
      "NDCG@5: 0.3896\n",
      "NDCG@10: 0.5042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [00:00, 25019.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping E:\\02456_Assignment\\pytorch\\test\\Dataset\\DUMP\\test_predictions\\NRMSDocVec-2024-12-09_00-42-25\\predictions.txt to E:\\02456_Assignment\\pytorch\\test\\Dataset\\DUMP\\test_predictions\\NRMSDocVec-2024-12-09_00-42-25\\NRMSDocVec-None-ebnerd_demo.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f\"\\nChecking test data distribution...\")\n",
    "print(\"Unique values in DEFAULT_CLICKED_ARTICLES_COL:\")\n",
    "print(df_test[DEFAULT_CLICKED_ARTICLES_COL].unique())\n",
    "print(\"Unique values in DEFAULT_INVIEW_ARTICLES_COL:\")\n",
    "print(df_test[DEFAULT_INVIEW_ARTICLES_COL].unique())\n",
    "\n",
    "\n",
    "print(\"\\nProcessing test chunks...\")\n",
    "df_pred_chunks = []\n",
    "\n",
    "with tqdm(total=len(df_test_chunks), desc=\"Processing chunks\", leave=True) as pbar:\n",
    "    for i, df_test_chunk in enumerate(df_test_chunks[CHUNKS_DONE:], start=1 + CHUNKS_DONE):\n",
    "        test_dataloader = NRMSDataLoader(\n",
    "            behaviors=df_test_chunk,\n",
    "            article_dict=article_mapping,\n",
    "            unknown_representation=\"zeros\",\n",
    "            history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            eval_mode=True,\n",
    "            batch_size=BATCH_SIZE_TEST_WO_B,\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(test_dataloader, batch_size=None, shuffle=False)\n",
    "        scores = predict_scores(model.scorer, test_loader, device)\n",
    "        \n",
    "\n",
    "        df_test_chunk = add_prediction_scores(df_test_chunk, scores.tolist())\n",
    "        df_test_chunk = df_test_chunk.with_columns([\n",
    "            pl.col(\"scores\")\n",
    "            .map_elements(lambda x: list(rank_predictions_by_score(x)), return_dtype=pl.List(pl.Float32))\n",
    "            .alias(\"ranked_scores\")\n",
    "        ])\n",
    "        \n",
    "        df_pred_chunks.append(df_test_chunk)\n",
    "        pbar.update(1)\n",
    "        \n",
    "\n",
    "        del test_dataloader, scores\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "print(\"\\nMerging prediction results...\")\n",
    "df_test = pl.concat(df_pred_chunks)\n",
    "\n",
    "print(\"Saving final predictions...\")\n",
    "df_test.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "    ARTIFACT_DIR.joinpath(\"test_predictions.parquet\")\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nEvaluating test set predictions...\")\n",
    "with tqdm(total=3, desc=\"Computing metrics\", leave=True) as pbar:\n",
    "\n",
    "    labels = [np.array(label, dtype=np.float32) for label in df_test[\"labels\"].to_list()]\n",
    "    scores = [np.array(score, dtype=np.float32) for score in df_test[\"scores\"].to_list()]\n",
    "    pbar.update(1)\n",
    "    \n",
    "\n",
    "    test_metrics = MetricEvaluator(\n",
    "        labels=labels,\n",
    "        predictions=scores,\n",
    "        metric_functions=[AucScore(), MrrScore(), NdcgScore(k=5), NdcgScore(k=10)]\n",
    "    )\n",
    "    test_results = test_metrics.evaluate()\n",
    "    pbar.update(1)\n",
    "    \n",
    "\n",
    "    write_json_file(test_results.evaluations, ARTIFACT_DIR.joinpath(\"test_metrics.json\"))\n",
    "    pbar.update(1)\n",
    "\n",
    "print(\"\\nTest Set Results:\")\n",
    "print(f\"AUC: {test_results.evaluations['auc']:.4f}\")\n",
    "print(f\"MRR: {test_results.evaluations['mrr']:.4f}\")\n",
    "print(f\"NDCG@5: {test_results.evaluations['ndcg@5']:.4f}\")\n",
    "print(f\"NDCG@10: {test_results.evaluations['ndcg@10']:.4f}\")\n",
    "\n",
    "\n",
    "if TEST_CHUNKS_DIR.exists() and TEST_CHUNKS_DIR.is_dir():\n",
    "    shutil.rmtree(TEST_CHUNKS_DIR)\n",
    "\n",
    "df_test = df_test.sort(DEFAULT_IMPRESSION_ID_COL)    \n",
    "write_submission_file(\n",
    "    impression_ids=df_test[DEFAULT_IMPRESSION_ID_COL],\n",
    "    prediction_scores=df_test[\"ranked_scores\"],\n",
    "    path=ARTIFACT_DIR.joinpath(\"predictions.txt\"),\n",
    "    filename_zip=f\"{MODEL_NAME}-{SEED}-{DATASPLIT}.zip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13536it [00:00, 170011.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping E:\\desktop\\test\\Dataset\\DUMP\\test_predictions\\NRMSDocVec-2024-12-07_21-49-03\\predictions.txt to E:\\desktop\\test\\Dataset\\DUMP\\test_predictions\\NRMSDocVec-2024-12-07_21-49-03\\NRMSDocVec-None-ebnerd_demo.zip\n"
     ]
    }
   ],
   "source": [
    "write_submission_file(\n",
    "    impression_ids=df_test[DEFAULT_IMPRESSION_ID_COL],\n",
    "    prediction_scores=df_test[\"ranked_scores\"],\n",
    "    path=ARTIFACT_DIR.joinpath(\"predictions.txt\"),\n",
    "    filename_zip=f\"{MODEL_NAME}-{SEED}-{DATASPLIT}.zip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nrms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
