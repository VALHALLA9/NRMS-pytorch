{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\tf_gpu_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "import shutil\n",
    "import gc\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import time\n",
    "from utils._constants import *\n",
    "from utils._behaviors import (\n",
    "    create_binary_labels_column,\n",
    "    sampling_strategy_wu2019,\n",
    "    add_prediction_scores,\n",
    "    truncate_history,\n",
    "    ebnerd_from_path,\n",
    ")\n",
    "from evaluation import MetricEvaluator, AucScore, NdcgScore, MrrScore\n",
    "from utils._python import (\n",
    "    write_submission_file,\n",
    "    rank_predictions_by_score,\n",
    "    write_json_file,\n",
    ")\n",
    "from utils._articles import create_article_id_to_value_mapping\n",
    "from utils._polars import split_df_chunks\n",
    "\n",
    "from models.model_config import (\n",
    "    hparams_nrms_docvec,\n",
    "    hparams_to_dict,\n",
    "    print_hparams,\n",
    ")\n",
    "from models.nrms_docvec import NRMSDocVec  # PyTorch version\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "cuDNN version: 8700\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ARGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.dataloader import NRMSDataLoader, NRMSDataLoaderPretransform\n",
    "PATH = Path(\"E:\\\\desktop\\\\test\\\\Dataset\").expanduser()\n",
    "SEED = None  \n",
    "DATASPLIT = \"ebnerd_demo\"\n",
    "DEBUG = False\n",
    "BS_TRAIN = 32\n",
    "BS_TEST = 32\n",
    "BATCH_SIZE_TEST_WO_B = 32\n",
    "BATCH_SIZE_TEST_W_B = 4\n",
    "HISTORY_SIZE = 20\n",
    "NPRATIO = 4\n",
    "EPOCHS = 1\n",
    "TRAIN_FRACTION = 1.0 if not DEBUG else 0.0001\n",
    "FRACTION_TEST = 0.001 if not DEBUG else 0.0001\n",
    "DOC_VEC_PATH = \"E:\\\\desktop\\\\test\\\\Dataset\\\\contrastive_vector.parquet\"\n",
    "\n",
    "\n",
    "NRMSLoader_training = NRMSDataLoaderPretransform\n",
    "\n",
    "\n",
    "\n",
    "model_func = \"NRMSDocVec\" \n",
    "hparams = {\n",
    "    \"title_size\": 768,\n",
    "    \"history_size\": 20,\n",
    "    \"head_num\": 16,\n",
    "    \"head_dim\": 16,\n",
    "    \"attention_hidden_dim\": 200,\n",
    "    \"newsencoder_units_per_layer\": [512, 512, 512],\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"cross_entropy_loss\",\n",
    "    \"dropout\": 0.2,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"newsencoder_l2_regularization\": 1e-4,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "df_articles = pl.read_parquet(DOC_VEC_PATH)\n",
    "\n",
    "\n",
    "def create_article_id_to_value_mapping(df, value_col):\n",
    "    return {row[0]: row[1] for row in df.select([df.columns[0], value_col]).iter_rows()}\n",
    "\n",
    "article_mapping = create_article_id_to_value_mapping(\n",
    "    df=df_articles, value_col=df_articles.columns[-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMP_DIR = PATH.joinpath(PATH, \"DUMP\")\n",
    "DUMP_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "DT_NOW = dt.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "MODEL_NAME = model_func\n",
    "MODEL_OUTPUT_NAME = f\"{MODEL_NAME}-{DT_NOW}\"\n",
    "\n",
    "ARTIFACT_DIR = DUMP_DIR.joinpath(\"test_predictions\", MODEL_OUTPUT_NAME)\n",
    "ARTIFACT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_WEIGHTS = DUMP_DIR.joinpath(f\"state_dict/{MODEL_OUTPUT_NAME}/weights.pt\")\n",
    "MODEL_WEIGHTS.parent.mkdir(parents=True, exist_ok=True)\n",
    "LOG_DIR = DUMP_DIR.joinpath(f\"runs/{MODEL_OUTPUT_NAME}\")\n",
    "\n",
    "TEST_CHUNKS_DIR = ARTIFACT_DIR.joinpath(\"test_chunks\")\n",
    "TEST_CHUNKS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "N_CHUNKS_TEST = 1\n",
    "CHUNKS_DONE = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We just want to load the necessary columns\n",
    "COLUMNS = [\n",
    "    DEFAULT_IMPRESSION_TIMESTAMP_COL,\n",
    "    DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    DEFAULT_INVIEW_ARTICLES_COL,\n",
    "    DEFAULT_CLICKED_ARTICLES_COL,\n",
    "    DEFAULT_IMPRESSION_ID_COL,\n",
    "    DEFAULT_USER_COL,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dict = {\n",
    "    \"data_path\": str(PATH),  \n",
    "    \"seed\": SEED,\n",
    "    \"datasplit\": DATASPLIT,\n",
    "    \"debug\": DEBUG,\n",
    "    \"bs_train\": BS_TRAIN,\n",
    "    \"bs_test\": BS_TEST,\n",
    "    \"batch_size_test_wo_b\": BATCH_SIZE_TEST_WO_B,\n",
    "    \"batch_size_test_w_b\": BATCH_SIZE_TEST_W_B,\n",
    "    \"history_size\": HISTORY_SIZE,\n",
    "    \"npratio\": NPRATIO,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"train_fraction\": TRAIN_FRACTION,\n",
    "    \"fraction_test\": FRACTION_TEST,\n",
    "    \"nrms_loader\": str(NRMSLoader_training),  \n",
    "    \"document_embeddings\": str(DOC_VEC_PATH),  \n",
    "    \"title_size\": hparams[\"title_size\"],\n",
    "    \"head_num\": hparams[\"head_num\"],\n",
    "    \"head_dim\": hparams[\"head_dim\"],\n",
    "    \"attention_hidden_dim\": hparams[\"attention_hidden_dim\"],\n",
    "    \"newsencoder_units_per_layer\": hparams[\"newsencoder_units_per_layer\"],\n",
    "    \"optimizer\": hparams[\"optimizer\"],\n",
    "    \"loss\": hparams[\"loss\"],\n",
    "    \"dropout\": hparams[\"dropout\"],\n",
    "    \"learning_rate\": hparams[\"learning_rate\"],\n",
    "    \"newsencoder_l2_regularization\": hparams[\"newsencoder_l2_regularization\"],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "write_json_file(params_dict, ARTIFACT_DIR.joinpath(f\"{MODEL_NAME}_argparser.json\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pl.concat(\n",
    "        [\n",
    "            ebnerd_from_path(\n",
    "                PATH.joinpath(DATASPLIT, \"train\"),\n",
    "                history_size=HISTORY_SIZE,\n",
    "                padding=0,\n",
    "            ),\n",
    "            ebnerd_from_path(\n",
    "                PATH.joinpath(DATASPLIT, \"validation\"),\n",
    "                history_size=HISTORY_SIZE,\n",
    "                padding=0,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    .sample(fraction=TRAIN_FRACTION, shuffle=True, seed=SEED)\n",
    "    .select(COLUMNS)\n",
    "    .pipe(\n",
    "        sampling_strategy_wu2019,\n",
    "        npratio=NPRATIO,\n",
    "        shuffle=True,\n",
    "        with_replacement=True,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    .pipe(create_binary_labels_column)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_dt = df[DEFAULT_IMPRESSION_TIMESTAMP_COL].dt.date().max() - dt.timedelta(days=1)\n",
    "df_train = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL).dt.date() < last_dt)\n",
    "df_validation = df.filter(pl.col(DEFAULT_IMPRESSION_TIMESTAMP_COL).dt.date() >= last_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NRMSLoader_training type: <class 'type'>\n"
     ]
    }
   ],
   "source": [
    "print(f\"NRMSLoader_training type: {type(NRMSLoader_training)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NRMSLoader_training(\n",
    "    behaviors=df_train,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=BS_TRAIN,\n",
    ")\n",
    "val_dataset = NRMSLoader_training(\n",
    "    behaviors=df_validation,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=False,\n",
    "    batch_size=BS_TRAIN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=None, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=None, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = NRMSDocVec(hparams=hparams_nrms_docvec, seed=42)  \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.model.to(device)\n",
    "model.scorer.to(device)\n",
    "optimizer = model.optimizer  \n",
    "criterion = model.criterion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=4, mode='max', restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.mode = mode\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "        self.best_state_dict = None\n",
    "\n",
    "    def step(self, score, model):\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.best_state_dict = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False\n",
    "        improve = (score > self.best_score) if self.mode == 'max' else (score < self.best_score)\n",
    "        if improve:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "            self.best_state_dict = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                if self.restore_best_weights:\n",
    "                    model.load_state_dict(self.best_state_dict)\n",
    "                return True\n",
    "            return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(patience=4, mode=\"max\", restore_best_weights=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.2, patience=2, min_lr=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc(model, dataloader):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    model.scorer.eval() \n",
    "    all_scores = []\n",
    "    all_labels = []\n",
    "    progress_bar = tqdm(total=len(dataloader), desc=\"Computing AUC\", dynamic_ncols=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for (his_input_title, pred_input_title), batch_y in dataloader:\n",
    "            # if not isinstance(his_input_title, torch.Tensor):\n",
    "            #     his_input_title = torch.from_numpy(his_input_title).float()\n",
    "            # his_input_title = his_input_title.to(device)\n",
    "\n",
    "            # if not isinstance(pred_input_title, torch.Tensor):\n",
    "            #     pred_input_title = torch.from_numpy(pred_input_title).float()\n",
    "            # pred_input_title = pred_input_title.to(device)\n",
    "\n",
    "            his_input_title = his_input_title.to(dtype=torch.float32, device=device)\n",
    "            pred_input_title = pred_input_title.to(dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "\n",
    "            scores = model.scorer(his_input_title, pred_input_title[:, 0:1, :])\n",
    "            scores = scores.cpu().numpy()\n",
    "            \n",
    "\n",
    "            labels = batch_y[:, 0].cpu().numpy()\n",
    "            \n",
    "            all_scores.extend(scores)\n",
    "            all_labels.extend(labels)\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "\n",
    "    progress_bar.close()\n",
    "    return roc_auc_score(all_labels, all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.model.train()\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    progress_bar = tqdm(total=len(dataloader), desc=\"Training\", dynamic_ncols=True)\n",
    "\n",
    "    for batch_idx, ((his_input_title, pred_input_title), batch_y) in enumerate(dataloader):\n",
    "\n",
    "        # if not isinstance(his_input_title, torch.Tensor):\n",
    "        #     his_input_title = torch.from_numpy(his_input_title).float()\n",
    "        # his_input_title = his_input_title.to(device)\n",
    "\n",
    "        # if not isinstance(pred_input_title, torch.Tensor):\n",
    "        #     pred_input_title = torch.from_numpy(pred_input_title).float()\n",
    "        # pred_input_title = pred_input_title.to(device)\n",
    "\n",
    "        # if not isinstance(batch_y, torch.Tensor):\n",
    "        #     batch_y = torch.from_numpy(batch_y).float()\n",
    "        # batch_y = batch_y.to(device)\n",
    "\n",
    "\n",
    "        his_input_title = his_input_title.to(dtype=torch.float32, device=device)\n",
    "        pred_input_title = pred_input_title.to(dtype=torch.float32, device=device)\n",
    "        batch_y = batch_y.to(dtype=torch.float32, device=device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model.model(his_input_title, pred_input_title)  \n",
    "        \n",
    "        # categorical_crossentropy\n",
    "        loss = -torch.sum(batch_y * torch.log(preds + 1e-10)) / batch_y.size(0)\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        grad_stats = []\n",
    "        for name, param in model.model.named_parameters():\n",
    "            if param.grad is not None:\n",
    "                grad_stats.append(f\"{name}: grad_mean={param.grad.mean().item():.6f}\")\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * len(batch_y)\n",
    "        count += len(batch_y)\n",
    "\n",
    "\n",
    "        progress_bar.set_postfix(\n",
    "            loss=f\"{loss.item():.6f}\",\n",
    "            grad_stats=\" | \".join(grad_stats[:2])\n",
    "        )\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    progress_bar.close()\n",
    "    return total_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.model.eval()\n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm(total=len(dataloader), desc=\"Evaluating\", dynamic_ncols=True)\n",
    "        for batch_idx, ((his_input_title, pred_input_title), batch_y) in enumerate(dataloader):\n",
    "            # if not isinstance(his_input_title, torch.Tensor):\n",
    "            #     his_input_title = torch.from_numpy(his_input_title).float()\n",
    "            # his_input_title = his_input_title.to(device)\n",
    "\n",
    "            # if not isinstance(pred_input_title, torch.Tensor):\n",
    "            #     pred_input_title = torch.from_numpy(pred_input_title).float()\n",
    "            # pred_input_title = pred_input_title.to(device)\n",
    "\n",
    "            # if not isinstance(batch_y, torch.Tensor):\n",
    "            #     batch_y = torch.from_numpy(batch_y).float()\n",
    "            # batch_y = batch_y.to(device)\n",
    "\n",
    "\n",
    "            his_input_title = his_input_title.to(dtype=torch.float32, device=device)\n",
    "            pred_input_title = pred_input_title.to(dtype=torch.float32, device=device)\n",
    "            batch_y = batch_y.to(dtype=torch.float32, device=device)\n",
    "\n",
    "            preds = model.model(his_input_title, pred_input_title)\n",
    "\n",
    "            loss = -torch.sum(batch_y * torch.log(preds + 1e-10)) / batch_y.size(0)\n",
    "            \n",
    "            total_loss += loss.item() * len(batch_y)\n",
    "            count += len(batch_y)\n",
    "\n",
    "            progress_bar.set_postfix(loss=f\"{loss.item():.6f}\")\n",
    "            progress_bar.update(1)\n",
    "        progress_bar.close()\n",
    "\n",
    "    return total_loss / count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.3.1+cu118\n",
      "CUDA available: True\n",
      "CUDA version: 11.8\n",
      "cuDNN version: 8700\n",
      "GPU Name: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA version:\", torch.version.cuda)\n",
    "    print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "    print(\"GPU Name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Current learning rate: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1426/1426 [00:13<00:00, 106.53it/s, grad_stats=userencoder.newsencoder.layers.0.0.weight: grad_mean=-0.000022 | userencoder.newsencoder.layers.0.0.bias: grad_mean=-0.006841, loss=1.715383]\n",
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:00<00:00, 245.87it/s, loss=1.789479]\n",
      "Computing AUC: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:00<00:00, 313.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Train Loss: 1.5336, Val Loss: 1.5150, Val AUC: 0.6241\n",
      "Best model updated\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    for param_group in optimizer.param_groups:\n",
    "        print(f\"Current learning rate: {param_group['lr']}\")\n",
    "\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    val_auc = compute_auc(model, val_loader)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    if val_auc > best_auc:\n",
    "        best_auc = val_auc\n",
    "        torch.save(model.model.state_dict(), MODEL_WEIGHTS)\n",
    "        print(\"Best model updated\")\n",
    "\n",
    "    scheduler.step(val_auc)\n",
    "    stop = early_stopping.step(val_auc, model.model)\n",
    "    if stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model: E:\\desktop\\test\\Dataset\\DUMP\\state_dict\\NRMSDocVec-2024-12-21_18-23-11\\weights.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NRMScorer(\n",
       "  (userencoder): UserEncoder(\n",
       "    (newsencoder): NewsEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (1-2): 2 x Sequential(\n",
       "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "          (1): ReLU()\n",
       "          (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (3): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (output_layer): Linear(in_features=512, out_features=256, bias=True)\n",
       "    )\n",
       "    (attention): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "    (att_layer): AttLayer2()\n",
       "  )\n",
       "  (newsencoder): NewsEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "      (1-2): 2 x Sequential(\n",
       "        (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (3): Dropout(p=0.2, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (output_layer): Linear(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"loading model: {MODEL_WEIGHTS}\")\n",
    "model.model.load_state_dict(torch.load(MODEL_WEIGHTS, map_location=device))\n",
    "model.model.eval()\n",
    "model.scorer.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set\n",
    "We'll use the validation set, as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating testset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Initiating testset...\")\n",
    "df_test = (\n",
    "    ebnerd_from_path(\n",
    "        PATH.joinpath(\"ebnerd_testset\", \"test\"),\n",
    "        history_size=HISTORY_SIZE,\n",
    "        padding=0,\n",
    "    )\n",
    "    .sample(fraction=FRACTION_TEST)\n",
    "    .with_columns(\n",
    "        pl.col(DEFAULT_INVIEW_ARTICLES_COL)\n",
    "        .list.first()\n",
    "        .alias(DEFAULT_CLICKED_ARTICLES_COL)\n",
    "    )\n",
    "    .select(COLUMNS + [DEFAULT_IS_BEYOND_ACCURACY_COL])\n",
    "    .with_columns(\n",
    "        pl.col(DEFAULT_INVIEW_ARTICLES_COL)\n",
    "        .list.eval(pl.element() * 0)\n",
    "        .alias(DEFAULT_LABELS_COL)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_wo_beyond = df_test.filter(~pl.col(DEFAULT_IS_BEYOND_ACCURACY_COL))\n",
    "df_test_w_beyond = df_test.filter(pl.col(DEFAULT_IS_BEYOND_ACCURACY_COL))\n",
    "\n",
    "df_test_chunks = split_df_chunks(df_test_wo_beyond, n_chunks=N_CHUNKS_TEST)\n",
    "df_pred_test_wo_beyond = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_scores(model_scorer, dataloader, device):\n",
    "    print(\"predict_scores: Entering function.\")\n",
    "    model_scorer.eval()\n",
    "    preds_all = []\n",
    "    batch_counter = 0  \n",
    "    progress_bar = tqdm(total=len(dataloader), desc=\"Processing batches\", dynamic_ncols=True)\n",
    "    start_time = time.time()  \n",
    "    for batch_idx, ((his_input_title, pred_input_title_one), _) in enumerate(dataloader):\n",
    "        try:\n",
    "            batch_start_time = time.time()  \n",
    "            # print(f\"Processing batch {batch_idx + 1}/{len(dataloader)}...\")\n",
    "\n",
    "            # if not isinstance(his_input_title, torch.Tensor):\n",
    "            #     his_input_title = torch.from_numpy(his_input_title).float()\n",
    "            # his_input_title = his_input_title.to(device)\n",
    "            # # print(f\"his_input_title shape: {his_input_title.shape}\")\n",
    "\n",
    "            # if not isinstance(pred_input_title_one, torch.Tensor):\n",
    "            #     pred_input_title_one = torch.from_numpy(pred_input_title_one).float()\n",
    "            # pred_input_title_one = pred_input_title_one.to(device)\n",
    "            # # print(f\"pred_input_title_one shape: {pred_input_title_one.shape}\")\n",
    "\n",
    "            his_input_title = his_input_title.to(dtype=torch.float32, device=device)\n",
    "            pred_input_title_one = pred_input_title_one.to(dtype=torch.float32, device=device)\n",
    "\n",
    "\n",
    "            scores = model_scorer(his_input_title, pred_input_title_one)\n",
    "            # print(f\"Scores shape: {scores.shape}\")\n",
    "            preds_all.extend(scores.cpu().tolist())\n",
    "\n",
    "\n",
    "            progress_bar.set_postfix(\n",
    "                batch=batch_idx + 1,\n",
    "                batch_size=his_input_title.shape[0]\n",
    "                # scores_shape=scores.shape\n",
    "            )\n",
    "            progress_bar.update(1)\n",
    "\n",
    "\n",
    "            # if batch_counter < 5:\n",
    "            #     print(f\"Batch {batch_idx + 1} first 5 scores: {scores[:5].cpu().tolist()}\")\n",
    "\n",
    "            batch_counter += 1\n",
    "            # print(f\"Batch {batch_idx + 1} processed in {time.time() - batch_start_time:.2f} seconds\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in batch {batch_idx + 1}: {e}\")\n",
    "            raise e\n",
    "\n",
    "    print(\"predict_scores: Completed processing all batches.\")\n",
    "    print(f\"Total batches processed: {batch_counter}\")\n",
    "    print(f\"Total predictions: {len(preds_all)}\")\n",
    "    print(f\"Total time taken: {time.time() - start_time:.2f} seconds\")\n",
    "    return np.array(preds_all, dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating testset without beyond-accuracy...\n",
      "Test chunk: 1/1\n",
      "Initializing NRMSDataLoader for test chunk...\n",
      "NRMSDataLoader initialized successfully.\n",
      "Initializing torch DataLoader...\n",
      "Torch DataLoader initialized successfully.\n",
      "Calling predict_scores...\n",
      "predict_scores: Entering function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 416/416 [00:42<00:00,  9.75it/s, batch=416, batch_size=125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_scores: Completed processing all batches.\n",
      "Total batches processed: 416\n",
      "Total predictions: 155670\n",
      "Total time taken: 42.67 seconds\n",
      "predict_scores completed successfully.\n",
      "Calling add_prediction_scores...\n",
      "add_prediction_scores completed successfully.\n",
      "Saving results to Parquet...\n",
      "Test chunk 1 processed and saved.\n",
      "Memory cleaned for chunk 1.\n"
     ]
    }
   ],
   "source": [
    "print(\"Initiating testset without beyond-accuracy...\")\n",
    "for i, df_test_chunk in enumerate(df_test_chunks[CHUNKS_DONE:], start=1 + CHUNKS_DONE):\n",
    "    print(f\"Test chunk: {i}/{len(df_test_chunks)}\")\n",
    "    \n",
    "\n",
    "    print(\"Initializing NRMSDataLoader for test chunk...\")\n",
    "    try:\n",
    "        test_dataloader_wo_b = NRMSDataLoader(\n",
    "            behaviors=df_test_chunk,\n",
    "            article_dict=article_mapping,\n",
    "            unknown_representation=\"zeros\",\n",
    "            history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "            eval_mode=True,\n",
    "            batch_size=BATCH_SIZE_TEST_WO_B,\n",
    "        )\n",
    "        print(\"NRMSDataLoader initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing NRMSDataLoader: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "    print(\"Initializing torch DataLoader...\")\n",
    "    try:\n",
    "        test_loader_wo_b = torch.utils.data.DataLoader(test_dataloader_wo_b, batch_size=None, shuffle=False)\n",
    "        print(\"Torch DataLoader initialized successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing torch DataLoader: {e}\")\n",
    "        raise e\n",
    "\n",
    "    print(\"Calling predict_scores...\")\n",
    "    try:\n",
    "        \n",
    "        scores = predict_scores(model.scorer, test_loader_wo_b, device)\n",
    "        print(\"predict_scores completed successfully.\")\n",
    "        # print(f\"First 5 scores: {scores[:5] if len(scores) > 5 else scores}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during predict_scores: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "    # print(\"Debugging df_test_chunk before add_prediction_scores...\")\n",
    "    # print(f\"Columns: {df_test_chunk.columns}\")\n",
    "    # print(f\"Shape: {df_test_chunk.shape}\")\n",
    "    # print(\"First 5 rows of df_test_chunk:\")\n",
    "    # print(df_test_chunk.head(5))\n",
    "\n",
    "\n",
    "    print(\"Calling add_prediction_scores...\")\n",
    "    try:\n",
    "        # print(\"Debugging df_test_chunk before add_prediction_scores:\")\n",
    "        # print(df_test_chunk.head(5))\n",
    "        # print(\"Columns:\", df_test_chunk.columns)\n",
    "        # print(\"Schema:\", df_test_chunk.schema)\n",
    "\n",
    "        # print(\"Debugging scores before add_prediction_scores:\")\n",
    "        # print(\"Scores type:\", type(scores))\n",
    "        # print(\"Scores shape:\", np.array(scores).shape if isinstance(scores, (list, np.ndarray)) else \"N/A\")\n",
    "        # print(\"First 5 scores:\", scores[:5])\n",
    "\n",
    "\n",
    "\n",
    "        df_test_chunk = add_prediction_scores(df_test_chunk, scores.tolist()).with_columns(\n",
    "            pl.col(\"scores\")\n",
    "            .map_elements(lambda x: list(rank_predictions_by_score(x)), return_dtype=pl.List(pl.Int32))\n",
    "            .alias(\"ranked_scores\")\n",
    "        )\n",
    "\n",
    "        print(\"add_prediction_scores completed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during add_prediction_scores: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "    print(\"Saving results to Parquet...\")\n",
    "    try:\n",
    "        df_test_chunk.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "            TEST_CHUNKS_DIR.joinpath(f\"pred_wo_ba_{i}.parquet\")\n",
    "        )\n",
    "        print(f\"Test chunk {i} processed and saved.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results to Parquet: {e}\")\n",
    "        raise e\n",
    "\n",
    "\n",
    "    df_pred_test_wo_beyond.append(df_test_chunk)\n",
    "    del df_test_chunk, test_dataloader_wo_b, scores\n",
    "    gc.collect()\n",
    "    print(f\"Memory cleaned for chunk {i}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initiating testset with beyond-accuracy...\n",
      "predict_scores: Entering function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:07<00:00,  7.85it/s, batch=61, batch_size=500] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict_scores: Completed processing all batches.\n",
      "Total batches processed: 61\n",
      "Total predictions: 60500\n",
      "Total time taken: 7.77 seconds\n",
      "Saving prediction results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_pred_test_wo_beyond = pl.concat(df_pred_test_wo_beyond)\n",
    "df_pred_test_wo_beyond.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "    TEST_CHUNKS_DIR.joinpath(\"pred_wo_ba.parquet\")\n",
    ")\n",
    "\n",
    "print(\"Initiating testset with beyond-accuracy...\")\n",
    "test_dataloader_w_b = NRMSDataLoader(\n",
    "    behaviors=df_test_w_beyond,\n",
    "    article_dict=article_mapping,\n",
    "    unknown_representation=\"zeros\",\n",
    "    history_column=DEFAULT_HISTORY_ARTICLE_ID_COL,\n",
    "    eval_mode=True,\n",
    "    batch_size=BATCH_SIZE_TEST_W_B,\n",
    ")\n",
    "test_loader_w_b = torch.utils.data.DataLoader(test_dataloader_w_b, batch_size=None, shuffle=False)\n",
    "scores = predict_scores(model.scorer, test_loader_w_b, device)\n",
    "df_pred_test_w_beyond = add_prediction_scores(df_test_w_beyond, scores.tolist()).with_columns(\n",
    "    pl.col(\"scores\")\n",
    "    .map_elements(lambda x: list(rank_predictions_by_score(x)), return_dtype=pl.List(pl.Int32))  \n",
    "    .alias(\"ranked_scores\")\n",
    ")\n",
    "df_pred_test_w_beyond.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "    TEST_CHUNKS_DIR.joinpath(\"pred_w_ba.parquet\")\n",
    ")\n",
    "\n",
    "print(\"Saving prediction results...\")\n",
    "df_test = pl.concat([df_pred_test_wo_beyond, df_pred_test_w_beyond])\n",
    "df_test.select(DEFAULT_IMPRESSION_ID_COL, \"ranked_scores\").write_parquet(\n",
    "    ARTIFACT_DIR.joinpath(\"test_predictions.parquet\")\n",
    ")\n",
    "\n",
    "if TEST_CHUNKS_DIR.exists() and TEST_CHUNKS_DIR.is_dir():\n",
    "    shutil.rmtree(TEST_CHUNKS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13536it [00:00, 167051.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zipping E:\\desktop\\test\\Dataset\\DUMP\\test_predictions\\NRMSDocVec-2024-12-21_18-23-11\\predictions.txt to E:\\desktop\\test\\Dataset\\DUMP\\test_predictions\\NRMSDocVec-2024-12-21_18-23-11\\NRMSDocVec-None-ebnerd_demo.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "write_submission_file(\n",
    "    impression_ids=df_test[DEFAULT_IMPRESSION_ID_COL],\n",
    "    prediction_scores=df_test[\"ranked_scores\"],\n",
    "    path=ARTIFACT_DIR.joinpath(\"predictions.txt\"),\n",
    "    filename_zip=f\"{MODEL_NAME}-{SEED}-{DATASPLIT}.zip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DONE ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
